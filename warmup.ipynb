{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-42f9f68ed8a3abc2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**<font color=\"red\">IMPORTANT</font>**: The notebook follows certain conventions when it comes to inserting your input in it:\n",
    "\n",
    "* Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\". Other cells that are not supposed to be edited are set read-only to prevent you from accidentally editing them. \n",
    "* Feel free to overwrite `NotImplemented` and `raise NotImplementedError()` when providing your solution, otherwise leave it as it is.\n",
    "* The comments starting with `#!` provide hints that need to be followed. For example, `#! A = NotImplemented` is a directive to define a variable `A`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "564631f3dc06439dd6826933f0715409",
     "grade": false,
     "grade_id": "cell-38070299edf92cbe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# ðŸ”¥ Torch Intro ðŸ”¥\n",
    "\n",
    "## What is PyTorch?\n",
    "PyTorch (or just torch) is a python deep learning library, by now getting widely adopted both in research and industry. Like most deep learning libraries it employs GPU accelaration; unlike most deep learning libraries, it supports dynamic computational graphs and deep python integration, enabling easy experimentation and code inspection. It provides high-level abstractions but also allows for low-level access to its primitives. You can read more about PyTorch at the official [site](https://pytorch.org/).\n",
    "\n",
    "## Installing PyTorch on your machine\n",
    "Install PyTorch by following the guidelines here [here](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> If you have an nVidia GPU, installing a CUDA version of PyTorch will allow you to utilize it, significantly speeding up computation.\n",
    "</div>\n",
    "\n",
    "# This Tutorial\n",
    "This tutorial will take you through PyTorch's main functionalities. It only aims to give you some insight on how to use PyTorch and is by no means a full tutorial on neural networks. Prior knowledge of neural netowrks and their inner workings (i.e. linear algebra, gradient-based optimization, back-propagation, etc.) will certainly prove useful. You are assumed to be largely fluent in python.\n",
    "\n",
    "# Table of Contents\n",
    "1. [Tensors](#1)\n",
    "    1. [Tensor Types](#1a)\n",
    "    2. [Instantiating Tensors](#1b)\n",
    "    3. [Basic Tensor Operations](#1c)\n",
    "    4. [Exercises](#1d)\n",
    "2. [Automatic Differentation](#2)\n",
    "    1. [Autograd](#2a)\n",
    "    2. [Exercises](#2b)\n",
    "3. [Neural Networks](#3)\n",
    "    1. [Custom Neural Networks](#3a)\n",
    "    2. [Loss Functions](#3b)\n",
    "    3. [Optimizers](#3c)\n",
    "    4. [Exercises](#3d)\n",
    "4. [Putting Everything Together](#4)\n",
    "\n",
    "\n",
    "For a more in-depth overview of PyTorch's capabilities, refer to the [official documentation](https://pytorch.org/docs/stable/index.html) (this link will prove handy for your assignments -- keep it close and use it often).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "96f23ae9bc2b957a86f7cc03a0f5d512",
     "grade": false,
     "grade_id": "cell-aa7b3f4f890f8b5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Getting started\n",
    "Let's verify your torch installation is working by trying to import it and printing its version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bafe609a3d4e3e4d383e21aff1d8bfbf",
     "grade": false,
     "grade_id": "cell-4ca1a4cfd8c804dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version = 1.10.1+cu113\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"torch version = {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "883692550276b367f3e95ad51ac67bee",
     "grade": false,
     "grade_id": "cell-cb7887e744fc8262",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='1'></a>\n",
    "## 1. Tensors\n",
    "\n",
    "A [Tensor](https://pytorch.org/docs/stable/tensors.html) is the building block of any PyTorch program; it is the abstraction that stores n-ary arrays of numbers (i.e. tensors) and provides various functionalities for processing them. \n",
    "\n",
    "<a id='1a'></a>\n",
    "### A. Tensor Types\n",
    "\n",
    "There are 16 types of Tensors, distinguished by their `dtypes` (the sort of numbers stored within them) and the `device` they can be accessed by (either GPU or CPU).\n",
    "\n",
    "The different Tensor types and their corresponding classes are shown below:\n",
    "\n",
    "| Description | dtype | CPU Tensor Class | GPU Tensor Class |\n",
    "| --- | --- | --- | --- |\n",
    "| Full precision float | `torch.float32` | `torch.FloatTensor` | `torch.cuda.FloatTensor`| \n",
    "| Half precision float | `torch.float16` | `torch.HalfTensor` | `torch.cuda.HalfTensor` |\n",
    "| Double precision float | `torch.float64` | `torch.DoubleTensor` | `torch.cuda.DoubleTensor` |\n",
    "| 8-bit unsigned integer | `torch.uint8` | `torch.ByteTensor` | `torch.cuda.ByteTensor` |\n",
    "| 8-bit signed integer | `torch.int8` | `torch.CharTensor` | `torch.CharTensor` |\n",
    "| 16-bit signed integer | `torch.int16` | `torch.ShortTensor` | `torch.cuda.ShortTensor` |\n",
    "| 32-bit signed integer | `torch.int32` | `torch.IntTensor` | `torch.cuda.IntTensor` |\n",
    "| 64-bit signed integer | `torch.int64` | `torch.LongTensor` | `torch.cuda.LongTensor` |\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b>\n",
    "Interaction between Tensors of different devices or dtypes is not permitted (so make sure you are consistent). \n",
    "</div>\n",
    "\n",
    "We are mostly interested in full precision floats and long integers (on either device), so we can forget about the rest of them for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3182316e2ca6c90d2ec82588976108a8",
     "grade": false,
     "grade_id": "cell-ffb680acc42ac1d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "For the sake of convenience, we will now specify the device used by the rest of the tutorial. If you have the cuda version installed but would rather not use it, change the snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d3640dfa50b5513cc0f81c1ec163db2d",
     "grade": false,
     "grade_id": "cell-e0997484db9771bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2756b490fd437ff092c0ddf1347a1209",
     "grade": false,
     "grade_id": "cell-a2de9fbbf53fde4a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='1b'></a>\n",
    "### B. Instantiating Tensors\n",
    "Tensors can be instantiated in a number of ways. When we want to construct a tensor of fixed dimensionality (shape) with random values, we may simply call the appropriate class constructor with the desired dimensionality as the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2f27b873ba32fb2e0c7f19db9506c7f0",
     "grade": false,
     "grade_id": "cell-1aa1d5c5044b27c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "my_first_long_tensor = torch.LongTensor(5)  # a vector of 5 longs\n",
    "print(my_first_long_tensor.shape)\n",
    "print(my_first_long_tensor.dtype)\n",
    "print(my_first_long_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "61ba025383358997b56ab53d93848c2b",
     "grade": false,
     "grade_id": "cell-042d93bf41fc8da7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_first_float_tensor = torch.FloatTensor(5, 5)  # a 5 by 5 matrix of floats\n",
    "print(my_first_float_tensor.shape)\n",
    "print(my_first_float_tensor.dtype)\n",
    "print(my_first_float_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "df17a08585d9302ea02a51f14c6864ea",
     "grade": false,
     "grade_id": "cell-4981308d62c31efb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "There are some useful shorthands for constructing tensors with commonly used values. Let's use some of them. For more details, look up the corresponding keywords in the pyTorch documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "47858281699ae02d1ad7b48b3e558122",
     "grade": false,
     "grade_id": "cell-d996d17476a88625",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = torch.zeros((2, 3, 4))  # a 2 by 3 by 4 tensor of zeros\n",
    "b = torch.ones(42)  # a vector of 42 ones\n",
    "c = torch.eye(3)  # a 3 by 3 identity matrix\n",
    "d = torch.rand((32, 10, 300))  # a 32 by 10 by 300 tensor of random numbers from a uniform distribution on the interval [0,1)\n",
    "e = torch.randint(low=0, high=10, size=(3, 3))  # a 3 by 3 matrix of random integers between 0 (incl.) and 10 (excl.)\n",
    "f = torch.arange(10)  # a vector containing the numbers 0 to 9 in ascending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "876e2a0de24615ee70f60671c8b8510b",
     "grade": false,
     "grade_id": "cell-e51949c7520041ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We can explicitly set the `dtype` and `device` arguments to specify the tensor's type and device (most often these default to torch.float and cpu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4199a4729091eae3dedffca093e585b9",
     "grade": false,
     "grade_id": "cell-66a453377dbc833a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a_long = torch.zeros((2, 3, 4), dtype=torch.long, device=device)\n",
    "f_float = torch.arange(10, dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5e819471043662723031d7c28713fcf5",
     "grade": false,
     "grade_id": "cell-89a81c73e1df2f9a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We can always query a tensor's contents, shape, dtype and device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9021ac53de315492d142bfa911593e35",
     "grade": false,
     "grade_id": "cell-df1ead5cccf99146",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for te in [a, a_long, b, c, d, e, f, f_float]:\n",
    "    print(te.shape)\n",
    "    print(te.dtype)\n",
    "    print(te.device)\n",
    "    # print(te)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "349ee084f74693705630df2334fb957b",
     "grade": false,
     "grade_id": "cell-0767b514648432fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We can create a tensor with the same size as an already existing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5957bbb217f701fa192ce36080a713e0",
     "grade": false,
     "grade_id": "cell-1e6cb3781a1bb257",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = torch.rand((32, 10, 300))\n",
    "\n",
    "like_d_rand = torch.rand_like(d) # a 32 by 10 by 300 tensor of random values\n",
    "print(like_d_rand.shape)\n",
    "\n",
    "like_d_ones = torch.ones_like(d) # a 32 by 10 by 300 tensor of ones \n",
    "print(like_d_ones.shape)\n",
    "\n",
    "like_d_zeros = torch.zeros_like(d) # a 32 by 10 by 300 tensor of zeros\n",
    "print(like_d_zeros.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4a9901d5fdbfc8bebeca09e3d76f42a1",
     "grade": false,
     "grade_id": "cell-58eb204d5652b0fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We can also specify the values of a tensor by passing a list (of lists*) of values during its construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d0f13602af4787be0033063b94c94b93",
     "grade": false,
     "grade_id": "cell-5c47e7dee849c5c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3], [4,5,6], [7,8,9]], device=device)\n",
    "# FYI, the following does the same for the continuous sequence of numbers\n",
    "# a = torch.arange(1, 10, device=device).reshape(3, 3)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "print(a.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2c25283b76ee60a9331bd363da098791",
     "grade": false,
     "grade_id": "cell-4de94b04a46c0d6f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Notice that PyTorch automatically assumed that the tensor we specified should be of type long (because we only provided integers as the tensor's contents). We could of course avoid this by manually specifying the dtype. Alternatively, we can alter the dtype and/or device post-construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fdbb80dc626d95a2c7ad0e40de4e16fa",
     "grade": false,
     "grade_id": "cell-0d2610edfbaab46d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = a.to(torch.float)\n",
    "print(a.dtype)\n",
    "a = a.to(\"cpu\")  # or alternatively, a = a.cpu()\n",
    "print(a.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "07e6518ac24e20f4baf567d758f0f4ea",
     "grade": false,
     "grade_id": "cell-9c8c50615322c070",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Finally, a torch tensor can also be directy constructed by (or converted to) a numpy array. Converting to a numpy array only works for cpu tensors. Note that when printing torch tensors, the default precision of floats is 4 while for numpy is 8. You can change this default behaviour of torch with [set_printpoints](https://pytorch.org/docs/master/generated/torch.set_printoptions.html#torch.set_printoptions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "24f762899c3e0daa80332b9d4a310b23",
     "grade": false,
     "grade_id": "cell-79e7fa12cef0a69d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a_np = np.random.random((2, 2))\n",
    "a_torch = torch.tensor(a_np, device=device)\n",
    "print(a_np)\n",
    "print()\n",
    "print(a_torch)\n",
    "print()\n",
    "a_np_2 = a_torch.cpu().numpy()\n",
    "print(a_np_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b752954af4f277e813c79272013a3f78",
     "grade": false,
     "grade_id": "cell-6107afd6d233f398",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# deleting above-defined variables\n",
    "del (\n",
    "    my_first_long_tensor, my_first_float_tensor, a, b, c, d, e, f, te, a_long, \n",
    "    f_float, like_d_rand, like_d_ones, like_d_zeros, a_np, a_torch, a_np_2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "35d66050c2feecef311f837d9fcf257d",
     "grade": false,
     "grade_id": "cell-2bb1ab8a0a1719e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='1c'></a>\n",
    "### C. Basic Tensor Operations\n",
    "Tensors and their contents are not hidden by the framework -- they are immediatelly accessible to us and we can interact with them in many ways, while being able to inspect the results of our actions. Let's walk through some of the most common usecases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fabadd44b197c51a3362e7ae20a04cf7",
     "grade": false,
     "grade_id": "cell-9cc8aee44452fddb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Indexing and Slicing\n",
    "Standard python indexing and slicing applies to torch tensors. Let's remember how that works -- first we will need a random matrix to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b00fac2395cd2d106a5c770c5d684bf1",
     "grade": false,
     "grade_id": "cell-17fa437acc460402",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4293, 0.1903, 0.8374],\n",
      "        [0.0252, 0.1757, 0.8026],\n",
      "        [0.5624, 0.3368, 0.0963],\n",
      "        [0.0417, 0.8957, 0.6450],\n",
      "        [0.2909, 0.0028, 0.9862]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((5, 3), device=device)  \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e68b852eedb61b653fb65ec16e1e18b9",
     "grade": false,
     "grade_id": "cell-a4f685e805f16496",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Remember!</b>\n",
    "Indexing starts from zero\n",
    "</div>\n",
    "\n",
    "Now let's try retrieving the 3rd item of the 1st row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fccb7aa1ecf552a0daf4148881635a21",
     "grade": false,
     "grade_id": "cell-f6d06cdd692fb099",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8374, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "b = a[0][2]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "292e3144c5f0d5958d8338ddf08cee78",
     "grade": false,
     "grade_id": "cell-f6b9b9ae5e30503e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Note that even though it's just one element, it's still a tensor (with 0 dimension) and not a float value. In order to get the value itself, we can call `item()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "54bc7fdcf50a6d3d6b4d82ebc0e2a1a4",
     "grade": false,
     "grade_id": "cell-8f38f79bb06bc76d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8374311327934265"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(b) is float)\n",
    "print(type(b.item()) is float)\n",
    "b.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b6d12fe6468a589ac4690e328246e984",
     "grade": false,
     "grade_id": "cell-0a867041f3b0c972",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "What if we wanted the first three rows of the matrix instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4d011c723d66e1583ac3875e2d9bf9e5",
     "grade": false,
     "grade_id": "cell-ed72f402fda4a2b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = a[:3]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "77d1d25eecb5d78e382e02f24c68a35f",
     "grade": false,
     "grade_id": "cell-c81109c37728ce64",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Or its third column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b36c0c4846f453d7825f2ff7e5484637",
     "grade": false,
     "grade_id": "cell-19cfbea79f2cace1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = a[:, 2]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "55ea0a81222ca0e26eeacc915a760501",
     "grade": false,
     "grade_id": "cell-00505b027a11bc50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Or every second element of the first column, starting from the second?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "914f3b92000748723a5175b73704f356",
     "grade": false,
     "grade_id": "cell-a1713665bcb6fca8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = a[1::2, 0]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f01290b6adeadc0cbf9336b316662b31",
     "grade": false,
     "grade_id": "cell-afc6956666e06672",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "But now in reverse, starting from the last?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9364fcf380d5b2392cc935ea55db926e",
     "grade": false,
     "grade_id": "cell-54b0c88c12a1b26d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "c = a[-1::-2, 0]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d4cc30098bf16f3338de584940a5e8e4",
     "grade": false,
     "grade_id": "cell-edcc4a8cb615f38f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Well, perhaps not... see [the issue](https://github.com/pytorch/pytorch/issues/604) about negative stepping in tensor slicing. One can achieve the reverse order with [flip](https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "73e2f83c1661d1c3dd1a5bdc3dab5731",
     "grade": false,
     "grade_id": "cell-ef782ac383697116",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can also select the maximum element from the tensor with `argmax` or the top K elements with `topk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "64bafa2e96701974335d7896f51c4e35",
     "grade": false,
     "grade_id": "cell-27af4e960ceecfd3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 0, 1, 2], device='cuda:0')\n",
      "\n",
      "torch.return_types.topk(\n",
      "values=tensor([[0.8374, 0.4293],\n",
      "        [0.8026, 0.1757],\n",
      "        [0.5624, 0.3368],\n",
      "        [0.8957, 0.6450],\n",
      "        [0.9862, 0.2909]], device='cuda:0'),\n",
      "indices=tensor([[2, 0],\n",
      "        [2, 1],\n",
      "        [0, 1],\n",
      "        [1, 2],\n",
      "        [2, 0]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "print(a.argmax(dim=1)) # returns the indices of the maximum values across the specified dimension\n",
    "print()\n",
    "print(a.topk(dim=1, k=2)) # returns both the indices and the values of the top K elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9f6c5aef944c120068dbd2e17590446a",
     "grade": false,
     "grade_id": "cell-07424d32de9ea4d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Value Assignment\n",
    "We can use the exact same scheme to assign values to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2df54c35724a6dfb1e54c2acd023aabc",
     "grade": false,
     "grade_id": "cell-f6b77f424e159143",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.1903, 0.8374],\n",
      "        [0.0252, 0.1757, 0.8026],\n",
      "        [0.5624, 0.3368, 0.0963],\n",
      "        [0.0417, 0.8957, 0.6450],\n",
      "        [0.2909, 0.0028, 0.9862]], device='cuda:0')\n",
      "\n",
      "tensor([[0.4600, 0.2284, 0.8060],\n",
      "        [0.4876, 0.7980, 0.3841],\n",
      "        [0.0650, 0.8973, 0.4593],\n",
      "        [0.2824, 0.9043, 0.5481],\n",
      "        [0.8349, 0.9966, 0.3432]], device='cuda:0')\n",
      "\n",
      "tensor([[0.0000, 0.1903, 0.8374],\n",
      "        [0.0650, 0.8973, 0.4593],\n",
      "        [0.5624, 0.3368, 0.0963],\n",
      "        [0.0417, 0.8957, 0.6450],\n",
      "        [0.2909, 0.0028, 0.9862]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Set the top left item of the matrix to zero.\n",
    "a[0,0] = 0\n",
    "print(a)\n",
    "print()\n",
    "# Construct another random matrix of the same shape.\n",
    "b = torch.rand_like(a, device=device)\n",
    "print(b)\n",
    "print()\n",
    "# Set the second row of matrix a to be the third row of matrix b.\n",
    "a[1] = b[2]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f23004d148902efaf685963b42a61389",
     "grade": false,
     "grade_id": "cell-605bb1b13f72cf34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "#### Elementwise Arithmetic\n",
    "Elementwise operations (most importantly comparison, addition, subtraction, multiplication and division) can be applied on tensors of compatible shapes (i.e. shapes that can be [broadcasted](https://pytorch.org/docs/stable/notes/broadcasting.html)). \n",
    "\n",
    "Two tensors are compatible if any of the two below conditions hold:\n",
    "* their shapes are the same \n",
    "* their trailing (i.e. last) N dimensions are the same (excluding missing dimensions and dimensions of size 1)\n",
    "\n",
    "Scalars (single values) are compatible with tensors of any shape. Let's see some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0ab12bc7b64ca11dcc71d2068381b05b",
     "grade": false,
     "grade_id": "cell-30f09b908965d58d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "First, some fresh tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "08d0d626a37717ffc5a4e44ad89493cf",
     "grade": false,
     "grade_id": "cell-fc3dc47f385b6f86",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = torch.zeros((2, 3, 4), device=device)  \n",
    "b = torch.ones((2, 3, 4), device=device)  \n",
    "c = torch.ones((3, 4), device=device)  \n",
    "d = torch.ones((2, 3, 1), device=device)\n",
    "e = torch.rand((4, 3, 2), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f8432d329838ed6d97a4278463712a65",
     "grade": false,
     "grade_id": "cell-e25e97dc0e1e6166",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We can add a scalar to tensor $a$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "deb033d6a54c67d0fb9efc4eaf564b9a",
     "grade": false,
     "grade_id": "cell-a03a8d8c1417eb21",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = a + 0.3\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b7bff9b84ea56c7180cf7cc950379e01",
     "grade": false,
     "grade_id": "cell-4b05400aa9b27fd5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We can subtract $b$ from $a$ (matching shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fd5c3aed632615fd052c4ac2fe4bef54",
     "grade": false,
     "grade_id": "cell-9b049904ac2db154",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = a - b\n",
    "print(f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e062b5fc41bf35e58d243fd0ce1eeaf6",
     "grade": false,
     "grade_id": "cell-58d513bd7d5b554e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We can elementwise multiply $a$ with $c$ (dimensions of $c$ are the same as the last dimensions of $a$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f2b80533f3f571a0379ced70e84ab8a6",
     "grade": false,
     "grade_id": "cell-a4d1e1b753642ad3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "g =  a * c\n",
    "print(g.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6db17b268c52a171a5495c6e2769ae9d",
     "grade": false,
     "grade_id": "cell-671766cebe514059",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We can elementwise divide $a$ by $d$ (the last dimension of $d$ is 1, the rest of the dimensions match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "43cbf80de2b05a25a52dfe0b50c3a8ee",
     "grade": false,
     "grade_id": "cell-dca3810715fb30c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "h = a / d\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "40897bb78089d26ca02d67ff42c6d3c2",
     "grade": false,
     "grade_id": "cell-daa66bd0396f3a4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We can elementwise raise $a$ to $d$ (the last dimension of $d$ is 1, the rest of the dimensions match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "55bf213a2abdadb5b30eb1e60d24d11c",
     "grade": false,
     "grade_id": "cell-58df9f562e36ba89",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = a**d\n",
    "print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6f72e5431208930e1c44e0d4a178ea6f",
     "grade": false,
     "grade_id": "cell-83f529f00f44e3af",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "And we can compare $a$ with any of $f$, $g$, $h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "14eb2781fcdf978c60ee3050b63643db",
     "grade": false,
     "grade_id": "cell-1f0b4687c42d57d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "j = a == f\n",
    "print(j.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. but torch complains when we try to do that with $e$ (the shapes are incompatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e == a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in doubt for what any of the elementwise operators actually do, try them out below on some tensors of your own making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenient shortcut for a particular sequence of elementwise operations is [`torch.where`](https://pytorch.org/docs/stable/generated/torch.where.html#torch.where). Given two tensors $x$ and $y$ and a boolean tensor $condition$, it compiles a new tensor selecting the elements from either $x$ or $y$, depending on $condition$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2)\n",
    "print(x)\n",
    "print(y)\n",
    "condition = torch.tensor([[True, False], [False, True]])\n",
    "z = torch.where(condition, x, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del a, b, c, d, e, f, g, h, i, j, x, y, z, condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Algebra\n",
    "Tensor algebra of course goes well beyond elementwise operations -- matrix multiplication is the bread and butter of machine learning, so we better get familiar with how torch does it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we begin by instantiating our matrices. Matrix multiplication is defined between matrices A and B of shapes [M, N] and [N, O] respectively and yields a matrix C of shape [M, O]. The torch function that implements matrix multiplication is `torch.mm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand([5, 3], device=device)\n",
    "B = torch.rand([3, 8], device=device)\n",
    "C = torch.mm(A, B)  # or alternatively, C = A @ B\n",
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we had several matrices (i.e. a batch of matrices) to each be multiplied with a matching B? We can use `torch.bmm` for efficient batch matrix multiplication.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Rather than writing slow and ugly `for` loops, employ array programming to write your machine learning code. This will make it much more concise and dramatically more efficient.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bA = torch.rand([128, 5, 3], device=device)\n",
    "bB = torch.rand([128, 3, 8], device=device)\n",
    "bC = torch.bmm(bA, bB)  # bC = bA @ bC also works here!\n",
    "print(bC.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b>\n",
    "Be careful not to confuse matrix multiplication `A@B` with the <a href=\"https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29\">Hadamard product</a> `A*B`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape Manipulation\n",
    "As we have seen, what we can do with tensors is largely dictated by their shapes. Adjusting a tensor's shape to allow for broadcasting or batching is therefore often necessary. The following functions should suffice for the bulk of shape manipulation tasks you might encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor tansposition is the generalization of matrix transposition. Since there are now more than 2 dimensions, we additionally need to specify the transposed dimensions. Take for instance a tensor of shape [M, N, O]. Converting it to a tensor of shape [N, M, O] requires transposing the first and second dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand([128, 5, 3], device=device)\n",
    "A = A.transpose(0, 1)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generalization of transposing is permutation of the tensor's dimensions. In calling `permute`, we must provide the new order of all the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand([128, 5, 3], device=device)\n",
    "A = A.permute(1, 0, 2)\n",
    "print(A.shape)\n",
    "A = A.permute(2, 0, 1)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also choose to create a reshaped view of a tensor; for instance we may collapse two or more tensor dimensions into one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand([128, 5, 3], device=device)\n",
    "A_collapsed = A.view(A.shape[0]*A.shape[1], A.shape[-1])\n",
    "print(A_collapsed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or expand one dimension into two or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_expanded = A_collapsed.view(128, 5, 3)\n",
    "print(A_expanded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convince ourselves that the back and forth between dimensions has left our tensor unaffected. First let's elementwise compare A with A_expanded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = A == A_expanded\n",
    "print(comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be okay! But what if there is a False somewhere in there? Python's [`all`](https://pytorch.org/docs/stable/generated/torch.all.html?highlight=all#torch.all) can be used directly on torch bools to help us here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all([all(row) for matrix in comp for row in matrix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or simply we can use [torch.equal](https://pytorch.org/docs/stable/generated/torch.equal.html) for comapring tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(A, A_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Views are useful, but as the name suggests they only change our view of a tensor. Different views of a tensor have the same number of elements; the view is just changing in what order these are read.\n",
    "\n",
    "For cases where we would like to repeat a tensor across one or more of its axes (actually creating a larger tensor), we can use the function of the same name. Let's consider a tensor of shape [M, N] which we would like to turn into a tensor that repeats itself K times across the first dimension (i.e. a tensor of shape [K $\\cdot$ M, N]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 12])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand([5, 12], device=device)\n",
    "A_repeat = A.repeat(3, 1)  # note that we are specifying the number of repeats per dimension\n",
    "print(A_repeat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to repeat a singleton dimension (i.e. a dimension of size 1), you can do that with the `expand` function which can be more efficient for large tensors since it does not allocate new memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 12])\n",
      "torch.Size([15, 12])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand([1, 12], device=device)\n",
    "A_exp = A.expand(15, 12)  # note that we are specifying the expected dimensions of the new tensor\n",
    "print(A_exp.shape)\n",
    "#\n",
    "A = torch.rand([1, 12], device=device)\n",
    "A_exp = A.expand(15, -1)  # -1 means not changing the size of that dimension\n",
    "print(A_exp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another convenient pair of functions is `squeeze`-`unsqueeze`. \n",
    "\n",
    "`squeeze` removes all the dimensions of size 1, or a specific dimension of size 1 if it's specified in the `dim` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 1])\n",
      "torch.Size([12])\n",
      "torch.Size([12, 1])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand([1, 12, 1], device=device)\n",
    "print(A.shape)\n",
    "\n",
    "A_squeezed = A.squeeze()\n",
    "print(A_squeezed.shape)\n",
    "\n",
    "A_squeezed_first = A.squeeze(dim=0)\n",
    "print(A_squeezed_first.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, `unsqueeze` inserts a new dimension of size 1 into a specified position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand([2, 12], device=device)\n",
    "print(A.shape)\n",
    "\n",
    "A_unsqueezed = A.unsqueeze(1)\n",
    "print(A_unsqueezed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Tensors\n",
    "Sometimes we may want to construct a big tensor out of two small ones. There's a few ways to accomplish that, but the most reliable one is through `torch.cat` ðŸˆ (shorthand for concatenate).\n",
    "\n",
    "Two tensors may be concatenated if they agree on all their dimensions, except for the concatenation dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand([4, 2], device=device)\n",
    "B = torch.rand([1, 2], device=device)\n",
    "C = torch.cat((A, B), dim=0)\n",
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (\n",
    "    A, B, C, comp, A_repeat, A_exp, A_expanded, A_collapsed, A_squeezed, \n",
    "    A_squeezed_first, A_unsqueezed, bA, bB, bC\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1d'></a>\n",
    "### Exercises\n",
    "It might be a good idea to take a short break here and recap on what we've seen before moving further. The mini-exercises below should help you test your grasp of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a tensor $A$ of shape [10, 10] containing random floats, and a tensor $B$ of the same shape where all its elements are equal to $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2c271d51d724112c523c75aaa8676beb",
     "grade": true,
     "grade_id": "cell-c91f4b96f56587d8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! A = NotImplemented\n",
    "#! B = NotImplemented\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $C = AB^T$, the matrix multiplication of $A$ with the transpose of $B$ and $D = A\\cdot B$, their elementwise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c5e73b531c80024a6e5de9c45dba0753",
     "grade": true,
     "grade_id": "cell-0ad645a31ad4dd02",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! C = NotImplemented\n",
    "#! D = NotImplemented\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C.shape)\n",
    "print(C)\n",
    "print(D.shape)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try comparing $C$ with $D$. Are they comparable? Are they equal? What is the dtype of their comparison?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "59ed9f43cef3fb64967fa84f5f5b3140",
     "grade": true,
     "grade_id": "cell-70e1c6fc933bf123",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiply $A$ by 4 to create $F$. Now set all elements of $F$ that are above $\\pi$ to zero.\n",
    "\n",
    "\n",
    "_Hint 1_: You can index a tensor with a boolean tensor of the same dimensionality\n",
    "\n",
    "_Hint 2_: You can set multiple indexed elements to a single value at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "602558e0ab366b4713c8b7dee3c0bfba",
     "grade": true,
     "grade_id": "cell-6c3e5f4fd2a2e394",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! F = NotImplemented\n",
    "#! F[NotImplemented] = 0\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The incomplete function below implements matrix multiplication with a for loop. Complete the function and call it with your $A$ and $B^T$ matrices as its arguments. The result should be the same as the matrix $C$ you computed before (with room for some numerical inaccuraccy)\n",
    "\n",
    "Note: You can use `torch.sum()` to compute the sum of a tensor (optionally specifying across which dimension)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> <a href=\"https://www.python.org/dev/peps/pep-0484/#rationale-and-goals\">Type Hints</a> may be used in python function and variable declarations to give them a type signature. These type signatures are not strict (you can still bypass them), but they can help you organize your code. Type hints of incomplete functions given during assignments will inform you of what we expect your function to accept and return.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b806439845eea1439a0ca5cd443ad12a",
     "grade": true,
     "grade_id": "cell-47774e9c724b0183",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_mm(A: torch.FloatTensor, B: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    assert A.shape[1] == B.shape[0]\n",
    "    assert (len(A.shape) == len(B.shape) == 2)\n",
    "    C = torch.zeros((A.shape[0], B.shape[1]), device=device)\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(B.shape[1]):\n",
    "            #! C[i,j] = NotImplemented\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "    return C\n",
    "    \n",
    "E = my_mm(A, B.transpose(1, 0))\n",
    "\n",
    "# Make sure the mean absolute difference between the two results is below 0.0001.\n",
    "assert torch.sum(torch.abs(E - C))/(E.shape[0]*E.shape[1]) < 1e-4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 2-dimensional tensor $S$ that consists of an instance of $A$ followed by two instances of $B$ followed by an instance of $A$ across its first dimension. What is the shape of $S$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b5dcb1508d050b1434e20dae94b37326",
     "grade": true,
     "grade_id": "cell-47a89260539fc314",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! S = NotImplemented\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(S.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape $S$ into a tensor of shape [20, 2, 10]. \n",
    "Then transpose this into a tensor of shape [2, 10, 20]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ca9b8982f27d9c6c4d036af47170f574",
     "grade": true,
     "grade_id": "cell-0c6a875ffdf30fba",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! S = NotImplemented\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(S.shape)\n",
    "#! S = NotImplemented\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "print(S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del A, B, C, D, E, F, S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Automatic Differentiation and Neural Networks\n",
    "We have so far seen some of torch's computational capabilities; its GPU accelaration and multitude of high level functions make it suitable for array processing and vector arithmetic. But torch is more than a faster numpy; its key components, and where most of the magic happens, are in its automatic differentiation mechanics and neural network libraries.\n",
    "\n",
    "<a id='2a'></a>\n",
    "### A. Autograd\n",
    "Each torch tensor carries a flag around with it, `requires_grad`, which establishes whether that tensor requires gradient computation. \n",
    "\n",
    "By default, tensors do not require grad unless specified to. Whenever a tensor that requires grad assumes a role in the construction of another tensor, the new tensor also requires grad.\n",
    "By dynamically tracking dependencies in the evolving computation graph, and utilizing this flag, torch is able to inform itself on which tensors need to be updated by gradient descent, and which do not (naturally, only tensors for which gradients are computed will be updated).\n",
    "\n",
    "Practically, by setting `requires_grad` to `False` we can _freeze_ (parts of) our functions, making them static.\n",
    "\n",
    "Let's see this in action by modeling a simple linear transformation $f(x): Ax$ from $x \\in \\mathbb{R}^5$  to $y \\in \\mathbb{R}^7$:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Recall that a matrix of shape [M, N] is a linear map <em>from</em> $\\mathbb{R}^N$ <em>to</em> $\\mathbb{R}^M$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6919, 0.6459, 0.7517, 0.2551, 0.2136],\n",
      "        [0.0316, 0.7695, 0.0466, 0.8732, 0.3765],\n",
      "        [0.5317, 0.5456, 0.4557, 0.1532, 0.3846],\n",
      "        [0.5271, 0.8347, 0.4891, 0.7046, 0.9799],\n",
      "        [0.1948, 0.5878, 0.1504, 0.8382, 0.6391],\n",
      "        [0.2774, 0.2241, 0.1369, 0.8511, 0.2390],\n",
      "        [0.0507, 0.1079, 0.8044, 0.2574, 0.1701]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand((7, 5), device=device)\n",
    "print(A)\n",
    "\n",
    "def f(x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    return A@x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function for some $x$, and check whether the result requires grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7345, 1.6712, 1.4721, 2.6951, 1.9270, 1.3936, 0.8450],\n",
      "       device='cuda:0')\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, device=device)\n",
    "\n",
    "y = f(x)\n",
    "print(y)\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not; what if the parameters of our function $f$ were trainable though?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "A.requires_grad = True\n",
    "y = f(x)\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of our linear transformation is now also trainable!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2b'></a>\n",
    "### Exercises\n",
    "Answer the next questions before you proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model an affine transformation $g(x) = Ax + \\beta$ from $x \\in \\mathbb{R}^3$ to $y \\in \\mathbb{R}^{12}$ as the composition of two functions, $f_1(x) = Ax$, $f_2(x) = x + \\beta$, such that $A$ requires grad but $\\beta$ does not.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> You can use `requires_grad: bool` as an optional argument during tensor construction. Can you guess its default value?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "982752b3f0a56e3d6502ff9bcc131e2d",
     "grade": true,
     "grade_id": "cell-33a6dc316f1b2725",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! A = NotImplemented\n",
    "#! beta = NotImplemented\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "def f_1(x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return NotImplemented\n",
    "\n",
    "def f_2(x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return NotImplemented\n",
    "\n",
    "x = torch.rand(3, device=device)\n",
    "\n",
    "w = f_1(x)\n",
    "y = f_2(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to figure out the answers on your own before verifying them with code.\n",
    "\n",
    "Let's assume $x$ is a fixed data sample, therefore does not require grad (we don't usually want to fit our data, but the function applied on the data!)\n",
    "\n",
    "* If $A$ requires grad but $\\beta$ doesn't, does $w$ require grad? Does $y$?\n",
    "\n",
    "* If $\\beta$ requires grad but $A$ doesn't, does $w$ require grad? Does $y$?\n",
    "\n",
    "What would that mean for $A$ and $\\beta$ during gradient descent?\n",
    "\n",
    "Verify your answers with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2c45cd9b6d447c82dd8a0f737135fbc6",
     "grade": true,
     "grade_id": "cell-fa90b0b77f21d5ab",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check grads of w and y \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b10f130bbafe2bce149c4dd904cc4196",
     "grade": true,
     "grade_id": "cell-6121a8fa10774c3a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check grads of w and y for a scenario where beta requires grad and A doesn't\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "54b23ccf2389e8e13676c4ff26e43408",
     "grade": true,
     "grade_id": "cell-7c84d825416dcead",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del A, beta, x, y, w, f, f_1, f_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "\n",
    "## 3. Neural Networks\n",
    "We are very close to defining our first neural network in torch. Torch includes a powerful neural network library, `torch.nn`, which allows us to create our own custom network flows, use highly optimized off-the-shelf implementations of most standard kinds of networks and compose different networks together.\n",
    "\n",
    "<a id='3a'></a>\n",
    "### A. Custom Neural Networks\n",
    "You have already implemented an affine transformation; a shallow feedforward network is simply such a transformation followed by a non-linearity. For the sake of familiarizing ourselves with custom torch networks, we will go through the process of defining such a network from scratch (you won't normally be doing this, but it's still beneficial to have an idea of what's happening at the low level before proceeding to the high level).\n",
    "\n",
    "The building block for a torch network is the `torch.nn.Module` class; we need to define our networks as objects inheriting that class. If we do so, we only need to implement two functions: `__init__()` and `forward()`. The first is responsible for registering the internal variables of our network, while the second specifies the kind of computation it actually performs.\n",
    "\n",
    "Let's see these in practice; we will define a shallow feedforward network implementing $f(x) = \\sigma(Wx + \\beta)$ from any input dimension to any output dimension, where $\\sigma$ the sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_first_network(torch.nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, device: str) -> None:\n",
    "        super(my_first_network, self).__init__()  # this is important! do not forget to call this\n",
    "        self.device = device\n",
    "        self.W = torch.nn.Parameter(torch.rand(out_features, in_features, device=self.device))\n",
    "        self.beta = torch.nn.Parameter(torch.rand(out_features, device=self.device))\n",
    "        \n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        return torch.sigmoid(self.W@x + self.beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Notice the use of torch.nn.Parameter. Wrapping the tensors that are parametric to our network's function in torch.nn.Parameter is crucial as it informs torch that these tensors need to be stored, updated and shared between different function calls.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test our first network class by instatiating an actual network and passing a random tensor through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = my_first_network(in_features=3, out_features=12, device=device)\n",
    "x = torch.rand(3, device=device)\n",
    "y = f(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Notice that we can use `f(x)` instead of `f.forward(x)` -- `forward` overloads the `__call__` method of the `torch.nn.Module` class, so the above statements are equivalent.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the abstraction for one layer, what's stopping us from instantiating a second network and composing the two into a deep network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_first_deep_network(torch.nn.Module):\n",
    "    def __init__(self, in_features: int, intermediate_features: int, out_features: int, device: str) -> None:\n",
    "        super(my_first_deep_network, self).__init__()\n",
    "        self.device = device\n",
    "        self.n_1 = my_first_network(\n",
    "            in_features=in_features, out_features=intermediate_features, device=self.device\n",
    "        )\n",
    "        self.n_2 = my_first_network(\n",
    "            in_features=intermediate_features, out_features=out_features, device=self.device\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        return self.n_2(self.n_1(x))\n",
    "    \n",
    "g = my_first_deep_network(in_features=3, intermediate_features=12, out_features=5, device=device)\n",
    "y = g(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very easy, right?\n",
    "\n",
    "Except we just did it the hard way! We could have used torch's pre-made `torch.nn.Linear`, the existing abstraction for single feedforward layers, and `torch.nn.Sequential`, the abstraction for composing sequences of networks. `Sequential` is initiated by an iterable of neural modules (not functions!), which are applied in the order specified.\n",
    "\n",
    "How would that have looked like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=3, out_features=12),\n",
    "    torch.nn.Sigmoid(), \n",
    "    torch.nn.Linear(in_features=12, out_features=5), \n",
    "    torch.nn.Sigmoid()  \n",
    ").to(device) \n",
    "\n",
    "y = h(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even easier!\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> When composing networks using the `Sequential` abstraction, you should make sure that each network's expected input shape matches the output shape of the previous network. The device conversion is applied recursively to each sub-module within a module, ensuring that all components of the network live happily in the same device. ðŸ  \n",
    "</div>\n",
    "\n",
    "So, why ever define our own networks if torch can do it for us? The answer is that very often (and very soon!) it might be the case that you'll need to write your own, potentially complex, computation flow, which won't necesserily be possible to rephrase as simple layer stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x, y, h, g, f, my_first_network, my_first_deep_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3b'></a>\n",
    "### B.  Loss Functions\n",
    "\n",
    "We have seen how to construct parametric, trainable functions and networks. Requiring a gradient and having a gradient are two different things, however. To obtain the gradients of our trainable parameters we need a loss function. The loss function is an indicator of how far off the network's output (prediction) is from the actual truth. Applying the chain rule, we may differentiate the loss value w.r.t. the model's parameters, populating the tensors' gradients in the process. \n",
    "\n",
    "As with networks, torch provides implementations for the commonly used loss functions but also allows us to write our own (as another neural module!).\n",
    "\n",
    "We will only experiment with an existing loss function, but first we need to construct some data to play with -- we can create a synthetic dataset of pairs $(x_i, y_i)$ where $x_i$ is a random number and $y_i = 3 \\cdot x_i - 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a tensor of shape 100, 1, i.e. 100 data points each of dimensionality 1\n",
    "x = torch.rand((100, 1), device=device) \n",
    "y = 3 * x - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to employ a linear network.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b>\n",
    "Don't mix up input/output dimensionality and number of data samples! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.nn.Linear(in_features=1, out_features=1).to(device)\n",
    "prediction = f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to define our loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each element of our batch (i.e. each of the 100 data samples) has its own MSE (mean squared error) w.r.t. its corresponding output. These unique losses are averaged into a single scalar by the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5040, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(y, prediction)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have computed the loss, we may use it to populate the parameters' gradients via a backward pass. This is automagically done by a simple call of `backward`. ðŸ§™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch provides a few different loss functions, each for a particular use case (the task and your output layer's activation function). Note that some loss functions are already implementing the network's output activation internally. Refer to the [documentation](https://pytorch.org/docs/stable/nn.html#loss-functions) for a detailed overview. In most cases, the cheatsheet below should contain the answer.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> At a loss which loss to use? Use this cheatsheet!\n",
    "</div>\n",
    "\n",
    "| Task | Activation | Loss Function |\n",
    "| --- | --- | --- |\n",
    "| K-class Classification | - | CrossEntropyLoss |\n",
    "| K-class Classification | LogSoftmax | NLLLoss |\n",
    "| K-class, Multi-Label Classification | - | BCEWithLogitsLoss |\n",
    "| K-class, Multi-Label Classification | LogSigmoid | BCELoss |\n",
    "| Continuous Regression | - | MSELoss |\n",
    "| Probability Distribution Fitting | LogSoftmax / LogSigmoid | KLDivLoss |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3c'></a>\n",
    "\n",
    "### C.  Optimizers\n",
    "Our struggles are slowly coming to an end. We have made a trainable network, we have computed the loss given the true output, and we have used the loss to populate the parameter gradients. The final thing to do is to use these gradients in order to update the parameter values. \n",
    "This is managed by an `Optimizer`. Gradient based optimizers are the norm for training neural networks; all of them are variants of stochastic gradient descent. Torch provides implementations of the classic optimizers. Regardless of which one of them is your favourite, the process always involves the same steps:\n",
    "1. Initiate the optimizer by letting it know which parameters it is going to be responsible for\n",
    "2. Iterate over your data, and:\n",
    "    2. Compute the loss\n",
    "    3. Back-propagate\n",
    "    4. Perform an optimization step\n",
    "    5. Zero out the gradients (so that they don't accumulate over optimization steps)\n",
    "\n",
    "Let's see them in action on our toy network and synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(f.parameters())  # initiate optimizer\n",
    "for t in range(5001):  # iterate\n",
    "    prediction = f(x)  # predict \n",
    "    loss = loss_fn(prediction, y)  # compute loss\n",
    "    loss.backward()  # backpropagate\n",
    "    opt.step()  # optimize\n",
    "    opt.zero_grad()  # reset gradients\n",
    "    if t % 500 == 0:\n",
    "        print(\"Iteration {} loss: {}\".format(t, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have trained your first torch network! ðŸŽ‰\n",
    "\n",
    "Remember that the network was trying to approximate $a$ and $b$ in $a \\cdot x + b$, which we set to $3 \\cdot x - 2$ when we created the dataset. You can now probe the network's inner parameters to see to what extent it figured out the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f.weight)\n",
    "print(f.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3d'></a>\n",
    "\n",
    "### D. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to a somewhat more realistic problem, it might be a good idea to get further acquainted with the basics. \n",
    "\n",
    "Let's take a quick look at a few different non-linear activations first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `arange` to construct a float tensor $x$ of values $0 \\dots 1000$ in ascending order. Then elementwise subtract $500$ and divide by $100$ to get a tensor of values $-5 \\dots 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1e0453271071e4ad5a8ba2b6464b62bb",
     "grade": true,
     "grade_id": "cell-e505c1d4bf3832b8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! x = NotImplemented\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the tensors $s = \\sigma(x)$, $t = tanh(x)$ and $r = ReLU(x)$ (refer to the documentation if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "382685a884defe4b5c8445d5cf772975",
     "grade": true,
     "grade_id": "cell-744d652f6651e2d2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "s = torch.sigmoid(x)\n",
    "#! t = NotImplemented\n",
    "#! r = NotImplemented\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert these to numpy arrays and plot the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(x.cpu().numpy(),s.cpu().numpy())\n",
    "plt.plot(x.cpu().numpy(),t.cpu().numpy())\n",
    "plt.plot(x.cpu().numpy(),r.cpu().numpy())\n",
    "plt.ylim((-2, 5))\n",
    "plt.legend([\"sigmoid\", \"tanh\", \"relu\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practice some more by solving the infamous [XOR problem](https://en.wikipedia.org/wiki/Exclusive_or) with a small deep network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], device=device, dtype=torch.float)\n",
    "Y = torch.tensor([0, 1, 1, 0], device=device, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `torch.nn.Sequential` to create a minimal deep network with 1 output dimension and 2 intermediate dimensions. Use ReLU as your intermediate layer activation. \n",
    "\n",
    "Picturing the problem as a classification over two classes, select an appropriate output activation and loss function (refer to the cheatsheet for aid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "38355799f5348b1584ffff9c67964b35",
     "grade": true,
     "grade_id": "cell-88cb7631423778b8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = torch.nn.Sequential(\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    ").to(device)\n",
    "\n",
    "#! loss_fn = NotImplemented\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate an optimizer for your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "a61e5fd06d34fda6ddaf5f1ec607b1bc",
     "grade": true,
     "grade_id": "cell-2826a2fd3560995b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! opt = NotImplemented\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform 5000 iterations of training, printing the loss as you go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5b3093949f84ad47c0ace6feeb7add62",
     "grade": true,
     "grade_id": "cell-d3794ad8a3f6ba9c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for t in range(1, 5001):  # iterate\n",
    "    #! P = NotImplemented  # predict \n",
    "    #! loss = NotImplemented  # compute loss\n",
    "    #! NotImplemented  # backpropagate\n",
    "    #! NotImplemented  # optimize\n",
    "    #! NotImplemented  # reset gradients\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    if t % 500 == 0:\n",
    "        print(\"Iteration {} loss: {}\".format(t, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might encounter some error here (e.g., shape, loss function, tensor type, value not in a rnage, etc.). If you do, don't panic! Read it, understand what the problem is, use google searhc, read documentation of certain loss functions if needed, and remember the shape manipulations operations covered in this notebook.\n",
    "\n",
    "Is the loss improving? If not, try toying around with the intermediate layer's width, the optimizer and its learning rate until your network can solve the problem.\n",
    "\n",
    "Check what the trained network now predicts on our input data. Is that what we would expect here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del f, loss_fn, opt, X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3d'></a>\n",
    "## 4. Putting Everything Together\n",
    "Time to hone our newly acquired torch skills! \n",
    "\n",
    "We will now put everything together and write an actual network on a real task. The code below is mostly complete, but some parts here and there are missing. You will be asked to fill those in, so pay attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, construct a two-layer network that implements the function $f: \\mathbb{R}^{300} \\to \\mathbb{R}$, such that:\n",
    "\n",
    "$f(x) = W_2(ReLU(W_1x + \\beta_1) + \\beta_2$\n",
    "\n",
    "where:\n",
    "* $W_1 \\in \\mathbb{R}^{100, 300}$\n",
    "* $W_2 \\in \\mathbb{R}^{1, 100}$ \n",
    "* $ \\beta_1 \\in \\mathbb{R}^{100}$ \n",
    "* $\\beta_2 \\in \\mathbb{R}^1$\n",
    "\n",
    "\n",
    "using `torch.nn.Sequential` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7ba0f467aef24ca91195f14d2a270613",
     "grade": true,
     "grade_id": "cell-01aa129f42feacb9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = torch.nn.Sequential(\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some use of this network on real data.\n",
    "We are going to open a data dump containing ~5500 baby names. Each name is associated with a label (either 0 for male, or 1 for female), and also a 300-dimensional vector. Representing words as dense vectors is standard practice in NLP; you will learn more about these vectors in your first assignment. For now, we will simply use them in an attempt to teach the network to distinguish between boy and girl babies given their names, while writing some useful code in the process. ðŸ‘¶ ðŸ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"name_data.p\", \"rb\") as fh:\n",
    "    names, vectors, labels = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does our data look like? Run the snippet below a couple of times to get an impression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.random.permutation(list(zip(names, labels)))[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vectors` is a list of numpy arrays, and `labels` is a list of integers. We will need to convert them to lists of FloatTensors. Since that is a lot of data, it is unlikely for them to all fit in the GPU, so we will use the RAM as a temporary storage regardless of your currently used device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or alternatively, vectors = [torch.tensor(vector) for vector in vectors]\n",
    "vectors = list(map(lambda x: torch.tensor(x, dtype=torch.float), vectors))\n",
    "labels = list(map(lambda x: torch.tensor(x, dtype=torch.float), labels))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on your entire dataset is bad practice; we should split the data into a training set and a validation set. We could either do it manually, or let `sklearn` do it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "names_train, names_val, X_train, X_val, Y_train, Y_val = train_test_split(names, vectors, labels, test_size=0.2)\n",
    "assert len(X_train) == len(Y_train) == len(names_train)\n",
    "assert len(X_val) == len(Y_val) == len(names_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split the data, we may convert them from a list of tensors into a big tensor. We could do that by using `view()` to expand the first dimension of each vector and then `cat()` to merge them, but an easier solution is `stack()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.stack(X_train)\n",
    "X_val = torch.stack(X_val)\n",
    "Y_train = torch.tensor(Y_train)\n",
    "Y_val = torch.tensor(Y_val)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our tensors in a sensible format, we may construct a Dataset (a storage unit for our data) and a DataLoader (a wrapper responsible for shuffling the data, iterating through it and converting it to batches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val, Y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)  # no need to shuffle the validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin and immediately stop an iteration through the training dataloader to get an idea of what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_x, batch_y in train_dataloader:\n",
    "    print(batch_x.shape)\n",
    "    print(batch_x.dtype)\n",
    "    print(batch_y.shape)\n",
    "    print(batch_y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good; batch_x is 32 300-dimensional vectors, and batch_y is 32 values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_batch()` is a function that takes a network, a batch of inputs, a batch of outputs, a loss function and an optimizer, runs the training routine on that batch and returns the loss value of that batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List\n",
    "\n",
    "# Callable is typed as Callable[[i1, i2, ..], o]\n",
    "# where i1, i2, .. are the input types and o is the output type.\n",
    "\n",
    "def train_batch(\n",
    "    network: torch.nn.Module,  # the network\n",
    "    X_batch: torch.FloatTensor,  # the X batch\n",
    "    Y_batch: torch.LongTensor,   # the Y batch\n",
    "    # a function from a FloatTensor (prediction) and a FloatTensor (Y) to a FloatTensor (the loss)\n",
    "    loss_fn: Callable[[torch.FloatTensor, torch.FloatTensor], torch.FloatTensor],  \n",
    "    # the optimizer\n",
    "    optimizer: torch.optim.Optimizer\n",
    ") -> float:\n",
    "    # Set the training mode.\n",
    "    network.train()\n",
    "    # Train.\n",
    "    prediction_batch = network(X_batch)  # forward pass\n",
    "    batch_loss = loss_fn(prediction_batch.view(-1), Y_batch)  # loss calculation\n",
    "    batch_loss.backward()  # gradient computation\n",
    "    optimizer.step()  # back-propagation\n",
    "    optimizer.zero_grad()  # gradient reset\n",
    "    return batch_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Several network components (e.g. dropout units) may behave differently during training and validation. We use `.train()` to inform the network that we are in training time.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b>\n",
    "The `batch_loss` tensor requires gradient (why?). It is important to return its contents with `.item()` rather than the tensor itself, otherwise we risk memory leak because of the accumulated gradient tracking.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_epoch()` is a function that takes a network, the training dataloader, a loss function and an optimizer. It iterates through the dataloader and is responsible for calling `train_batch()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    network: torch.nn.Module, \n",
    "    dataloader: DataLoader,\n",
    "    loss_fn: Callable[[torch.FloatTensor, torch.FloatTensor], torch.FloatTensor],\n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    device: str\n",
    ") -> float:\n",
    "    # Set the initial loss value.\n",
    "    loss = 0.\n",
    "    # Iterate over the batches in the dataloader.\n",
    "    for i, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        x_batch = x_batch.to(device)  # convert back to your chosen device\n",
    "        y_batch = y_batch.to(device)\n",
    "        loss += train_batch(\n",
    "            network=network, X_batch=x_batch, Y_batch=y_batch, loss_fn=loss_fn, optimizer=optimizer\n",
    "        )\n",
    "    loss /= (i+1) # divide the loss by the number of batches for consistency \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your turn; fill in the missing parts of `eval_batch()`, a function that takes a network, a batch of inputs, a batch of outputs and a loss function, and computes the loss of that batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fbcc27a61dcf29fdbebee1dc2ff587a7",
     "grade": true,
     "grade_id": "cell-11a6b34645f56976",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_batch(\n",
    "    network: torch.nn.Module,  # the network\n",
    "    X_batch: torch.FloatTensor,  # the X batch\n",
    "    Y_batch: torch.LongTensor,   # the Y batch\n",
    "    loss_fn: Callable[[torch.FloatTensor, torch.LongTensor], torch.FloatTensor]\n",
    ") -> float:\n",
    "    # Set the evaluation mode.\n",
    "    network.eval()\n",
    "    #\n",
    "    with torch.no_grad():\n",
    "        #! NotImplemented\n",
    "    #! return NotImplemente\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Notice that we use `.eval()` to inform the network we are in validation time. Notice also the `no_grad()` context; this is telling torch that it doesn't need to bother with gradient tracking momentarily, providing a significant speed-up for the current session.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eval_epoch()` is basically the same as `train_epoch()`, aside from the lack of an optimizer. Fill it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b1346e230a8ce16533e4e5838f15614b",
     "grade": true,
     "grade_id": "cell-2507cd102d033437",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_epoch(\n",
    "    network: torch.nn.Module, \n",
    "    dataloader: DataLoader,\n",
    "    loss_fn: Callable[[torch.FloatTensor, torch.LongTensor], torch.FloatTensor],\n",
    "    device: str\n",
    ") -> float:\n",
    "    #! NotImplemented\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make an auxilliary `infer_batch()` function; the forward pass gives us the final layer's output, but we might be more interested in the predicted class rather than its probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_batch(\n",
    "    network: torch.nn.Module, \n",
    "    batch_x: torch.FloatTensor, \n",
    "    device: str\n",
    ") -> torch.LongTensor:\n",
    "    # First apply the sigmoid activation \n",
    "    # (since it is implemented by the loss function rather than the network itself).\n",
    "    sigm = torch.sigmoid(network(batch_x.to(device)))\n",
    "    # Round the result.\n",
    "    classes = torch.round(sigm)\n",
    "    # Detach it from the computation graph (we no longer care about its gradients).\n",
    "    classes = classes.detach()\n",
    "    # Cast the result into a LongTensor and return.\n",
    "    return classes.to(torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing before we can finally train; we need a loss function and an optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(f.parameters(), lr=1e-05)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for t in range(NUM_EPOCHS):\n",
    "    train_loss = train_epoch(f, train_dataloader, optimizer=opt, loss_fn=loss_fn, device=device)\n",
    "    val_loss = eval_epoch(f, val_dataloader, loss_fn, device=device)\n",
    "    \n",
    "    print(\"Epoch {}\".format(t))\n",
    "    print(\" Training Loss: {}\".format(train_loss))\n",
    "    print(\" Validation Loss: {}\".format(val_loss))\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may plot the losses to get an idea of what the learning curve looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.legend([\"Training\", \"Validation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's bring this to an end by labeling all of our validation data and printing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for x_batch, _ in val_dataloader:\n",
    "    p_batch = infer_batch(f, x_batch, device).cpu().numpy().tolist()\n",
    "    predictions.extend(p_batch)\n",
    "    \n",
    "from pprint import pprint\n",
    "pprint(list(zip(names_val, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very convincing! What are these word vectors and how are they helping our network predict baby genders?  ðŸ¤”\n",
    "\n",
    "Do the first assignment and find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
